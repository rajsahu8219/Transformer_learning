{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code I want to implemnet simple transformer model to read shakespeare and spit out some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us first get the test\n",
    "# Let's open a webpage, download the text file and read it\n",
    "import urllib.request\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = urllib.request.urlopen(url)\n",
    "data_in = response.read().decode('utf8')\n",
    "\n",
    "# print first 1000 characters of the raw text\n",
    "# print(data_in[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple character level tokenizer\n",
    "# First, let us create a list of all the unique characters in the text and sort them\n",
    "chars = sorted(list(set(data_in)))\n",
    "#print(chars)\n",
    "\n",
    "# the vocab size is the number of unique characters\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping to map each character to a unique integer\n",
    "chtoi = {c: i for i, c in enumerate(chars)}\n",
    "itoch = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# create an encode function to encode the text into a list of integers ( took help from Andrej's video)\n",
    "def encode(text):\n",
    "    return [chtoi[c] for c in text]\n",
    "\n",
    "# create a decode function to decode the list of integers into text\n",
    "def decode(text):\n",
    "    return ''.join([itoch[i] for i in text])\n",
    "\n",
    "# okay, now let us encode the text into a list of integers\n",
    "data_en = encode(data_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, time to start training\n",
    "# first, put the data into a torch tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor of the encoded text\n",
    "data = torch.tensor(data_en, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create self attention class\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        \n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        \n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        \n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        \n",
    "        # split embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        \n",
    "        values = self.values(values) # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys) # (N, key_len, heads, heads_dim)\n",
    "        queries = self.queries(queries) # (N, query_len, heads, heads_dim)\n",
    "        \n",
    "        # energy = QK^T\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        \n",
    "        # queries shape: (N, query_len, heads, heads_dim)\n",
    "        # keys transpose shape: (N, heads, heads_dim, key_len)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        # attention = softmax(energy / (d ** (1/2)))\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "        \n",
    "        # out = attention * V\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N   \n",
    "            , query_len \n",
    "            , self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        # concatenate heads together and put through final linear layer\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# create a transformer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
